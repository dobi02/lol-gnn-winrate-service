from __future__ import annotations

import os
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from typing import Any, Dict, List, Tuple, Optional
import traceback
from mlflow.tracking import MlflowClient

from .schemas import (
    SpectatorPredictRequest,
    SpectatorPredictResponse,
    DiscordPredictRequest,
    DiscordPredictResponse,
)
from .predictor import LoLPredictor
from .services import build_model_input_from_spectator, fetch_spectator_and_enrichment_from_riot_id
from .model_loader import load_artifacts_meta
from . import db

# Global model store
ml_models = {}


def get_production_run_id(experiment_name: str) -> str:
    """MLflowë¥¼ ê²€ìƒ‰í•˜ì—¬ status íƒœê·¸ê°€ productionì¸ ê°€ì¥ ìµœì‹  ëª¨ë¸ì˜ run_idë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    client = MlflowClient()

    # 1. ì‹¤í—˜(Experiment) ID ì°¾ê¸°
    experiment = client.get_experiment_by_name(experiment_name)
    if experiment is None:
        raise ValueError(f"âŒ MLflowì—ì„œ '{experiment_name}' ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ì¡°ê±´(íƒœê·¸)ìœ¼ë¡œ Run ê²€ìƒ‰ (ìµœì‹ ìˆœ ì •ë ¬)
    # ê²€ìƒ‰ ì¡°ê±´: tags.status ê°€ 'production' ì¸ ê²ƒ
    query = "tags.status = 'production'"
    runs = client.search_runs(
        experiment_ids=[experiment.experiment_id],
        filter_string=query,
        order_by=["start_time DESC"],  # ê°€ì¥ ìµœê·¼ì— productionìœ¼ë¡œ ì§€ì •ëœ ê²ƒ 1ê°œ
        max_results=1
    )

    if not runs:
        raise ValueError(f"âŒ '{experiment_name}' ì‹¤í—˜ì—ì„œ '{query}' íƒœê·¸ë¥¼ ê°€ì§„ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 3. ì°¾ì€ run_id ë°˜í™˜
    return runs[0].info.run_id


@asynccontextmanager
async def lifespan(app: FastAPI):
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
    device = os.getenv("DEVICE", "cpu")
    experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "LoL_Win_Prediction_v1")

    # (ì„ íƒ) DB ì´ˆê¸°í™”
    # db.init_db(os.getenv('POSTGRES_DSN'))

    try:
        # â˜… MLflow ê²€ìƒ‰ì„ í†µí•´ Production ëª¨ë¸ì˜ run_id íšë“ â˜…
        print(f"ğŸ” MLflowì—ì„œ '{experiment_name}'ì˜ Production ëª¨ë¸ì„ ê²€ìƒ‰ ì¤‘...")
        run_id = get_production_run_id(experiment_name)
        print(f"ğŸ¯ Production ëª¨ë¸ ë°œê²¬! (Run ID: {run_id})")

        # ì°¾ì€ run_idë¥¼ Predictorì— ë„˜ê²¨ì„œ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ìˆ˜í–‰
        predictor = LoLPredictor(run_id=run_id, device=device)
        ml_models["predictor"] = predictor

        ml_models["meta"] = {
            "model": {
                "experiment": experiment_name,
                "run_id": run_id,
            },
            "status": "production"
        }
        print(f"âœ… Predictor ì„œë¹™ ì¤€ë¹„ ì™„ë£Œ")

    except Exception as e:
        print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨í•˜ë”ë¼ë„ ì„œë²„ëŠ” ë„ìš°ë ¤ë©´ ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šê³  Mockìœ¼ë¡œ ë„˜ê¸°ê±°ë‚˜ ì—ëŸ¬ ì²˜ë¦¬
        pass

    yield
    # db.close_db()
    ml_models.clear()


app = FastAPI(
    title="LoL GNN Winrate Service",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get("/health")
async def health():
    ready = "predictor" in ml_models
    return {"status": "ok" if ready else "not_ready"}


@app.get("/meta")
async def meta():
    if "meta" not in ml_models:
        raise HTTPException(status_code=503, detail="Model metadata is not ready")
    return ml_models["meta"]


@app.post("/predict/by-spectator", response_model=SpectatorPredictResponse)
async def predict_by_spectator(request: SpectatorPredictRequest):
    predictor: LoLPredictor = ml_models.get("predictor")
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model is not ready")

    warnings = []

    # Build model input (Graph/Data) from spectator payload
    try:
        graph_obj, build_warnings = await build_model_input_from_spectator(
            spectator=request.spectator,
            enrichment=request.enrichment,
        )
        warnings.extend(build_warnings)
    except Exception as e:
        raise HTTPException(status_code=422, detail=f"Preprocessing failed: {e}")

    # Predict
    try:
        win_rate_100 = predictor.predict_team100_win_rate(graph_obj)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {e}")

    # Ensure complementary probability
    win_rate_100 = float(win_rate_100)
    win_rate_200 = float(max(0.0, min(1.0, 1.0 - win_rate_100)))

    meta = ml_models.get("meta", {})
    model_meta = meta.get("model", meta) if isinstance(meta, dict) else {}

    return SpectatorPredictResponse(
        win_rate_team_100=win_rate_100,
        win_rate_team_200=win_rate_200,
        model=model_meta,
        warnings=warnings,
    )


@app.post("/predict")
async def predict_deprecated():
    raise HTTPException(
        status_code=410,
        detail="Deprecated. Use POST /predict/by-spectator with Riot spectator payload.",
    )

def _map_upstream_errors_to_http(e: Exception) -> HTTPException:
    """1~4 ë‹¨ê³„ì—ì„œ ë°œìƒí•˜ëŠ” ì—…ìŠ¤íŠ¸ë¦¼(Riot/DB) ì˜¤ë¥˜ë¥¼ HTTPExceptionìœ¼ë¡œ ë§¤í•‘."""

    print("âŒ [DEBUG] Upstream Error Caught:")
    traceback.print_exc()

    msg = str(e)
    if "riot_id must be in" in msg:
        return HTTPException(status_code=422, detail=msg)
    if "Riot API error 404" in msg:
        return HTTPException(status_code=404, detail="Summoner is not currently in an active game")
    if "RIOT_API_KEY is not set" in msg:
        return HTTPException(status_code=500, detail="Server is missing RIOT_API_KEY")
    return HTTPException(status_code=502, detail=f"Upstream error: {msg}")


async def resolve_spectator_and_enrichment_or_raise(
    *,
    riot_id: str,
    platform_id: str,
    use_history: bool,
    history_count: int,
) -> Tuple[Dict[str, Any], Dict[str, Any], List[str]]:
    """
    (1~4) Discord ì…ë ¥(riot_id)ë¡œë¶€í„°:
      1) riot_id -> puuid
      2) puuid -> active game spectator payload
      3) DB cache for participant history/mastery
      4) Cache miss -> Riot fetch
    ë¥¼ ìˆ˜í–‰í•˜ê³  (spectator_payload, enrichment, warnings)ë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ HTTPExceptionì„ raise.
    """
    try:
        spectator_payload, enrichment, warnings = await fetch_spectator_and_enrichment_from_riot_id(
            riot_id=riot_id,
            platform_id=platform_id,
            use_history=use_history,
            history_count=history_count,
        )
        return spectator_payload, enrichment, warnings
    except Exception as e:
        raise _map_upstream_errors_to_http(e)


async def predict_and_format_response_or_raise(
    *,
    predictor: "LoLPredictor",
    spectator_payload: Dict[str, Any],
    enrichment: Dict[str, Any],
    warnings: List[str],
    request_platform_id: str,
    meta: Any,
) -> "DiscordPredictResponse":
    """
    (5~6)
      5) Build model input and predict
      6) Return to bot (DiscordPredictResponse)
    ë¥¼ ìˆ˜í–‰.
    ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ HTTPExceptionì„ raise.
    """
    # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš© (spectator -> graph)
    spectator_obj = SpectatorPredictRequest(spectator=spectator_payload, enrichment=enrichment).spectator
    graph_obj, more_warnings = await build_model_input_from_spectator(spectator_obj, enrichment=enrichment)
    warnings.extend(more_warnings)

    try:
        win100 = predictor.predict_team100_win_rate(graph_obj)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {type(e).__name__}: {e}")

    win100 = float(win100)
    win200 = float(max(0.0, min(1.0, 1.0 - win100)))

    return DiscordPredictResponse(
        win_rate_team_100=win100,
        win_rate_team_200=win200,
        model=meta,
        warnings=warnings,
        game_id=int(spectator_payload.get("gameId")) if spectator_payload.get("gameId") is not None else None,
        platform_id=str(spectator_payload.get("platformId")) if spectator_payload.get("platformId") is not None else request_platform_id,
    )


# ---- ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì•„ë˜ì²˜ëŸ¼ â€œë‘ í•¨ìˆ˜â€ë¥¼ í˜¸ì¶œë§Œ í•˜ë„ë¡ ì–‡ê²Œ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤. ----
@app.post("/predict/from-discord", response_model=DiscordPredictResponse)
async def predict_from_discord(request: DiscordPredictRequest):
    predictor: LoLPredictor = ml_models.get("predictor")
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model is not ready")

    # (1~4)
    spectator_payload, enrichment, warnings = await resolve_spectator_and_enrichment_or_raise(
        riot_id=request.riot_id,
        platform_id=request.platform_id,
        use_history=request.use_history,
        history_count=request.history_count,
    )

    # (5~6)
    meta = ml_models.get("meta")
    return await predict_and_format_response_or_raise(
        predictor=predictor,
        spectator_payload=spectator_payload,
        enrichment=enrichment,
        warnings=warnings,
        request_platform_id=request.platform_id,
        meta=meta,
    )
